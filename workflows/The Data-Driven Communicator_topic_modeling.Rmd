---
title: "DDC Topic Modeling"
author: "Alexander Rose"
date: "3/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Environment Set Up

```{r package management, echo = FALSE, message=FALSE}
# As we prepare our workspace, let's begin by ensuring that we all of the packages that we will be making use of during our analysis
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, knitr, kableExtra, ggthemes, tidytext, tictoc, wordcloud, reshape2, igraph, ggraph, topicmodels, tm, RTextTools, SnowballC, stringr)

```

```{r}
set.seed(30303)
```


```{r}
setwd([INSERT])

RH_top100_posts.data <- as_tibble(read_csv("raw_data/RH_WSB_KW_search_posts.csv"))

RH_top100_comments.data <- as_tibble(read_csv("raw_data/KW_Search_RH_posts_comments.csv"))
```

```{r}
#simplifying the OG posts data and concatenate title and body
RH_posts_text.data <- RH_top100_posts.data %>% 
  mutate(text = paste(title, body, sep = " ")
         ,parent_id = NA
         ,type = "post") %>% 
  select(id, parent_id, score, type, timestamp, text)
```

```{r}
#simplifying the OG comments data
RH_comments_text.data <- RH_top100_comments.data %>% 
  mutate(text = body
         ,type = "comment") %>% 
  select(id, parent_id, score, type, timestamp, text)
```

```{r}
#union of our two data sets
RH_text.data <- union_all(RH_posts_text.data, RH_comments_text.data)
```


## Topic Modeling

Using a technique called topic modeling we can get a rough idea of the skeletal structure of the conversation. We will group comments into clusters based on their similarity to one another. We can then analyze frequently occurring terms in these clusters to try to understand their topic. We an then intuitively apply a label to each of these topics and use this information to provide a rough sketch of the conversational themes that are most prevalent in students responses.

```{r}
RH_text_TM.data <- RH_text.data
```

```{r}
#words
RH_text_TM.data$text <- RH_text_TM.data$text %>% 
  str_replace(regex("fuck", ignore_case = TRUE), "f.ck")

RH_text_TM.data$text <- RH_text_TM.data$text %>% 
  str_replace(regex("shit", ignore_case = TRUE), "sh.t")

RH_text_TM.data$text <- RH_text_TM.data$text %>% 
  str_replace(regex("retard", ignore_case = TRUE), "ret.rd")

RH_text_TM.data$text <- RH_text_TM.data$text %>% 
  str_replace(regex("dick", ignore_case = TRUE), "d.ck")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
#corpus <- sat_text.df[1:2]
corpus <- VCorpus(VectorSource(RH_text_TM.data$text)) # now we are Creating a Document Term Matrix.

ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, ct, "/|@|\\|:")
corpus<- tm_map(corpus, stripWhitespace) # remove white space

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers) # remove numbers
corpus <- tm_map(corpus, removePunctuation) # remove punctuations
corpus <- tm_map(corpus, removeWords, stopwords(kind="en")) # removes common stopwords
corpus <- tm_map(corpus, removeWords, c("a","the")) # removes customer stopwords



dtm <- DocumentTermMatrix(corpus)
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document # remove dtm rows with no words i.e., tweets that have no words
# after preprocessing text.
dtm <- dtm[rowTotals> 0, ]

```

```{r}
lda <- LDA(dtm, k = 6, method = "Gibbs", control = NULL)
# the method used for fitting can be either "VEM" or "Gibbs"
topics <- tidy(lda, matrix = "beta") # beta is the topic-word density
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>% # top_n picks 10 topics. ungroup() %>%
  arrange(topic, -beta)
top_terms %>%
  mutate(term = reorder(term, beta)) %>% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + coord_flip()
```

---
title: "The Data-Driven Communicator: A Crash Course in Social Listening, Analyzing
  Brand Voice, Understanding Sentiment and More"
author: "Alexander Rose"
date: "2/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Environment Set Up

```{r package management, echo = FALSE, message=FALSE}
# As we prepare our workspace, let's begin by ensuring that we all of the packages that we will be making use of during our analysis
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, knitr, kableExtra, ggthemes, tidytext, tictoc, wordcloud, reshape2, igraph, ggraph, topicmodels, tm, RTextTools, SnowballC, stringr, lubridate, scales)

```

```{r}
set.seed(30303)
```



```{r}
setwd([INSERT])

RH_top100_posts.data <- as_tibble(read_csv("raw_data/RH_WSB_KW_search_posts.csv"))

RH_top100_comments.data <- as_tibble(read_csv("raw_data/KW_Search_RH_posts_comments.csv"))
```

```{r}
#simplifying the OG posts data and concatenate title and body
RH_posts_text.data <- RH_top100_posts.data %>% 
  mutate(text = paste(title, body, sep = " ")
         ,parent_id = NA
         ,type = "post") %>% 
  select(id, parent_id, score, type, timestamp, text)
```

```{r}
#simplifying the OG comments data
RH_comments_text.data <- RH_top100_comments.data %>% 
  mutate(text = body
         ,type = "comment") %>% 
  select(id, parent_id, score, type, timestamp, text)
```

```{r}
#union of our two data sets
RH_text.data <- union_all(RH_posts_text.data, RH_comments_text.data)
```

### Custom Clean up of text

```{r}
#removing system notifications that are placed between square brakets example: [deleted]
RH_text.data$text <- gsub("\\[.*?\\]", "", RH_text.data$text)
```

```{r}
#removing most URLs
RH_text.data$text <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", RH_text.data$text)
```

```{r}
#More URL cleanup. This is overly aggressive but for time constraints i'm okay with it.
RH_text.data$text <- gsub("\\(.*?\\)", "", RH_text.data$text)
```

```{r}
RH_text.data$text <- gsub("(RH| rh |rh )", " robinhood ", RH_text.data$text)
```

```{r}
RH_text.data$text <- gsub("(robin hood)", "robinhood", RH_text.data$text)
```

```{r}
RH_text.data$text <- gsub("(GME|gme|Gme)", " gamestop ", RH_text.data$text)
```

```{r}
#removing empty rows
RH_text.data <- RH_text.data %>% 
  filter(RH_text.data$text != "")
```


```{r}
RH_text.tidy <- RH_text.data %>% unnest_tokens(word, text)
```

```{r}
RH_text.tidy <- RH_text.tidy %>% anti_join(stop_words)
```

#### Sentiment over time

```{r}
RH_text.timesentiment <- RH_text.tidy %>% 
  mutate(timestamp = ymd_hms(timestamp)
         ,date = date(timestamp)
         ,hour = hour(timestamp)
         ,seq_date = paste(date, hour, sep = "-")
         ) %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(seq_date) %>% 
  count(seq_date, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
ggplot(RH_text.timesentiment, aes(seq_date, sentiment)) +
  geom_col(show.legend = FALSE, fill = "#FFAB40") +
  labs(title = "Bing Sentiment Over Time"
       ,y = "Sentiment"
       ,x = "DateTime (by the Hour)") +
   theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

```{r}
RH_text.daysentiment <- RH_text.tidy %>% 
  mutate(timestamp = ymd_hms(timestamp)
         ,date = date(timestamp)
         ) %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(date) %>% 
  count(date, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
ggplot(RH_text.daysentiment, aes(date, sentiment)) +
  geom_col(show.legend = FALSE, fill = "#FFAB40") +
  labs(title = "Bing Sentiment Over Time"
       ,y = "Sentiment"
       ,x = "Date")
```


#### Stemming?

```{r}
RH_text.stem <- RH_text.tidy
RH_text.stem$word <- RH_text.stem$word %>% 
  wordStem()
```


#### Addressing Vulgarity

```{r}
#Stems
RH_text.stem$word <- gsub("fuck", "f.ck", RH_text.stem$word)
```

```{r}
#words
#RH_text.tidy$word <- gsub("fuck", "f.ck", RH_text.tidy$word)
#RH_text.tidy$word <- gsub("shit", "sh.t", RH_text.tidy$word)
#RH_text.tidy$word <- gsub("retard", "ret.rd", RH_text.tidy$word)
#RH_text.tidy$word <- gsub("dick", "d.ck", RH_text.tidy$word)
```


```{r}
RH_text.stem %>%
  count(word, sort = TRUE) %>%
  filter(n > 2000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, color = "#FFAB40")) +
  labs(title = "Word Stems Sorted by Frequency"
       ,y = "Number of Occurances"
       ,caption = "Stems combine similar words to their root; e.g. 'buy' and 'buying' become 'bui'") +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  guides(color = FALSE)
```


```{r}
bing_sentiments <- get_sentiments("bing")

bing_word_counts <- RH_text.tidy %>%
  inner_join(bing_sentiments)
```

```{r}
#dealing with vulgarity

bing_word_counts$word <- gsub("fucking", "f.ck", bing_word_counts$word)
bing_word_counts$word <- gsub("fuck", "f.ck", bing_word_counts$word)
bing_word_counts$word <- gsub("shit", "sh.t", bing_word_counts$word)
bing_word_counts$word <- gsub("retards", "ret.rd", bing_word_counts$word)
bing_word_counts$word <- gsub("retarded", "ret.rd", bing_word_counts$word)
bing_word_counts$word <- gsub("retard", "ret.rd", bing_word_counts$word)
bing_word_counts$word <- gsub("dick", "d.ck", bing_word_counts$word)
```

```{r}
bing_word_counts <- bing_word_counts %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


```{r}
bing_word_counts %>%
  filter(word != "fidelity" & word != "hedge") %>% 
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()+
  scale_fill_manual(values=c("#FFAB40", "#999999"))

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
bing_word_counts %>%
  filter(word != "fidelity" & word != "hedge") %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#FFAB40", "#999999"),
                   max.words = 100)
```

## N-gram Analysis

```{r}
RH_text_ngram.data <- RH_text.data
```

```{r}
#words
RH_text_ngram.data$text <- RH_text_ngram.data$text %>% 
  str_replace(regex("fuck", ignore_case = TRUE), "f.ck")

RH_text_ngram.data$text <- RH_text_ngram.data$text %>% 
  str_replace(regex("shit", ignore_case = TRUE), "sh.t")

RH_text_ngram.data$text <- RH_text_ngram.data$text %>% 
  str_replace(regex("retard", ignore_case = TRUE), "ret.rd")

RH_text_ngram.data$text <- RH_text_ngram.data$text %>% 
  str_replace(regex("dick", ignore_case = TRUE), "d.ck")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
RH_text_bigrams <- RH_text_ngram.data %>% 
  unnest_tokens(bigram,text,token = "ngrams", n=2)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
bigrams_separated <- RH_text_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_counts <- bigram_counts[2:nrow(bigram_counts),]

bigram_counts %>% 
  head(20) %>% 
  kbl(caption = "20 most common bigrams") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
bigram_graph <- bigram_counts %>%
  filter(n > 60) %>%
  graph_from_data_frame()

#bigram_graph
```
While the above table highlights some of the most frequently occurring themes we can also try to map out the more of the conversation by looking at a network graph of the words that were used.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "#FFAB40", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```


We can glean additional insight be focusing in on the most commonly occurring 3-word phrases.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
RH_text_trigrams <- RH_text.data %>% 
  unnest_tokens(trigram,text,token = "ngrams", n=3)
```

```{r, echo=FALSE}
trigrams_separated <- RH_text_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# new trigram counts:
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

trigram_counts <- trigram_counts[2:nrow(trigram_counts),]

#trigram_counts
trigram_counts %>% 
  head(20) %>% 
  kbl(caption = "20 most common trigrams") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```









